{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PoYDugixZ03G",
    "outputId": "a50a3dce-81c4-4379-c052-e0583f62294a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4U6NlmkvbHJ"
   },
   "source": [
    "# Installing Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifzSl9bXvwkK"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "La9CXPhOwAPJ"
   },
   "source": [
    "## load the pre-trained BERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7pdjNYYv2u5"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d5lUuIDwPku",
    "outputId": "56201a1f-82f3-4d4f-d953-d5857ca30916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23mSe6VVwikW"
   },
   "source": [
    "We have the main BERT model, a dropout layer to prevent overfitting, and finally a dense layer for classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB2VkH3XwvYJ"
   },
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pjtNN7zfv2l8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APatS-xC4EFN"
   },
   "source": [
    "## Large Movie Review Dataset\n",
    "\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8artPclexSkU"
   },
   "source": [
    "### Get the Data from the Stanford Repo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmaCPPLI0X1U"
   },
   "source": [
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UIMBlrbv2O0",
    "outputId": "f8a0a817-7d7a-4244-af3c-2520956da94a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
    "                                  origin=URL,\n",
    "                                  untar=True,\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbp91dfb5YZ-"
   },
   "source": [
    "### Preprossesing and pereparing the data for Bert model\n",
    "\n",
    "@Credit:\n",
    "Orhan G. Yalçın\n",
    "Nov 28, 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXlisOs8xghR",
    "outputId": "aaf5e24c-d4d7-4704-fbd9-f77e976c2536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labeledBow.feat', 'pos', 'urls_unsup.txt', 'urls_pos.txt', 'urls_neg.txt', 'unsupBow.feat', 'neg']\n"
     ]
    }
   ],
   "source": [
    "# The shutil module offers a number of high-level \n",
    "# operations on files and collections of files.\n",
    "import os\n",
    "import shutil\n",
    "# Create main directory path (\"/aclImdb\")\n",
    "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "# Create sub directory path (\"/aclImdb/train\")\n",
    "train_dir = os.path.join(main_dir, 'train')\n",
    "# Remove unsup folder since this is a supervised learning task\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "# View the final train folder\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5hq6XauxzRj"
   },
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lotNluoqxweU",
    "outputId": "43cd1778-a1bc-4d91-ac65-c41ca3b5058a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='training', seed=123)\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='validation', seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "E8068JZHyCTo",
    "outputId": "470dbaf9-f416-42b1-ace6-f9950e772fba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canadian director Vincenzo Natali took the art...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I gave this film 10 not because it is a superb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I admit to being somewhat jaded about the movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For a long time, 'The Menagerie' was my favori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A truly frightening film. Feels as if it were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DATA_COLUMN LABEL_COLUMN\n",
       "0  Canadian director Vincenzo Natali took the art...            1\n",
       "1  I gave this film 10 not because it is a superb...            1\n",
       "2  I admit to being somewhat jaded about the movi...            1\n",
       "3  For a long time, 'The Menagerie' was my favori...            1\n",
       "4  A truly frightening film. Feels as if it were ...            0"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in train.take(1):\n",
    "  train_feat = i[0].numpy()\n",
    "  train_lab = i[1].numpy()\n",
    "\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "train['DATA_COLUMN'] = train['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0jbXzHAzyCC1",
    "outputId": "c824f8b4-6968-4e09-d181-97fc9d2fcb57"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can't believe that so much talent can be was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie blows - let's get that straight rig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The saddest thing about this \"tribute\" is that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm only rating this film as a 3 out of pity b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Something surprised me about this movie - it w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DATA_COLUMN LABEL_COLUMN\n",
       "0  I can't believe that so much talent can be was...            0\n",
       "1  This movie blows - let's get that straight rig...            0\n",
       "2  The saddest thing about this \"tribute\" is that...            0\n",
       "3  I'm only rating this film as a 3 out of pity b...            0\n",
       "4  Something surprised me about this movie - it w...            1"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in test.take(1):\n",
    "  test_feat = j[0].numpy()\n",
    "  test_lab = j[1].numpy()\n",
    "\n",
    "test = pd.DataFrame([test_feat, test_lab]).T\n",
    "test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "test['DATA_COLUMN'] = test['DATA_COLUMN'].str.decode(\"utf-8\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B343Flr-yLhN",
    "outputId": "457bf79e-fff3-4f62-e0d8-ffb7d0138e81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputExample(guid=None, text_a='Hello, world', text_b=None, label=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputExample(guid=None,\n",
    "             text_a = \"Hello, world\",\n",
    "             text_b = None,\n",
    "             label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ffVAg2VbyLSK"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "\n",
    "  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
    "                                                                           test, \n",
    "                                                                           'DATA_COLUMN', \n",
    "                                                                           'LABEL_COLUMN')\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJY9s1vYybr6",
    "outputId": "0f71528c-f978-496c-d0c5-00c27f221f2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayINsmTl7t4D"
   },
   "source": [
    "* Adam as our optimizer :\n",
    "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n",
    "\n",
    "* CategoricalCrossentropy as our loss function:\n",
    "Categorical crossentropy is a loss function that is used in multi-class classification tasks. These are tasks where an example can only belong to one out of many possible categories, and the model must decide which one.\n",
    "\n",
    "* SparseCategoricalAccuracy as our accuracy metric :\n",
    "This metric creates two local variables, `total` and `count` that are used to\n",
    "compute the frequency with which `y_pred` matches `y_true`. This frequency is\n",
    "ultimately returned as `sparse categorical accuracy`: an idempotent operation\n",
    "that simply divides `total` by `count`.\n",
    "\n",
    "* epochs ( here only 2):\n",
    "Epoch refers to one cycle through the full training dataset, or the number of times that a learning algorithm is going to update the weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofvRWaamybfr",
    "outputId": "4408b181-b112-42b5-d775-bb44b05680ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4aab449ec0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4aab449ec0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f4ac6cf8c20> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function wrap at 0x7f4ac6cf8c20> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "   1250/Unknown - 1076s 819ms/step - loss: 0.3480 - accuracy: 0.8388WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1250/1250 [==============================] - 1121s 856ms/step - loss: 0.3479 - accuracy: 0.8388 - val_loss: 0.3267 - val_accuracy: 0.8796\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 1071s 857ms/step - loss: 0.0986 - accuracy: 0.9661 - val_loss: 0.5525 - val_accuracy: 0.8798\n",
      "CPU times: user 13min 58s, sys: 12min 28s, total: 26min 26s\n",
      "Wall time: 36min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f496a86ba50>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "%time model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utTBowMAJ1TG"
   },
   "source": [
    "## Testing the model to predict one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "pVnh7NLVybV3"
   },
   "outputs": [],
   "source": [
    "pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vr1LGRwZybLb",
    "outputId": "3e0739f4-c3a8-47ab-9767-70c6d12c414f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good : \n",
      " Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "  print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1steO3Wm_Xo7",
    "outputId": "ddecedfb-fdaa-4fee-dd5e-8fb507eb3569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[4.9489609e-04, 9.9950504e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3h9KZWWv7gbu",
    "outputId": "d0bd0670-162e-4885-ac37-b4e786d8c6c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput([('logits',\n",
       "                             <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-3.8668618,  3.743806 ]], dtype=float32)>)])"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Nh_l2G58grs",
    "outputId": "fb6f0e9c-c9f0-4222-b7ec-1ab99ff4d612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(tf_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "PxiqvHuJKQPd",
    "outputId": "d71e30aa-48d8-430e-c2e1-7bb4c5f46775"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[label[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnfIgPx-KaaI"
   },
   "source": [
    "## Reading the data from one we saved in Jupyter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rMHpfF2LC_Mu"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/reviews_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkRD4vPxC_QG",
    "outputId": "b2820f1f-69ad-47b1-e822-021709d4f2e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ‘Jakob’s Wife’ Review: A Cheap and Cheerless V...\n",
       "1     ‘City of Lies’ Review: Johnny Depp Solves the ...\n",
       "2     ‘Deadly Illusions’ Review: Kristin Davis Hires...\n",
       "3     Snyder Cut Justice League review: Still a mess...\n",
       "4     The Courier movie review: An intimate portraya...\n",
       "5     ‘Falcon And The Winter Soldier’ Review: Thanks...\n",
       "6     ‘Wojnarowicz’ Review: A Queer Biography as Bra...\n",
       "7     Mumbai Saga Public Review: Interesting After T...\n",
       "8     Movie Review | 'The Courier' is deliberate, in...\n",
       "9     FilmWeek: ‘The Courier,’ ‘Zack Snyder’s Justic...\n",
       "10                   The Feast (SXSW 2021 Movie Review)\n",
       "11    Review: 'Zack Snyder's Justice League' has arr...\n",
       "12    ‘Deadly Illusions’ Review: Kristin Davis Hires...\n",
       "13    Mosagallu movie review: This Vishnu Manchu, Ka...\n",
       "14    Sandeep Aur Pinky Faraar movie review: Two wor...\n",
       "15    Gaia starring Monique Rockman - (SXSW Horror M...\n",
       "16    Justice League: Why The Industry Can't Pretend...\n",
       "17    SXSW Review: See You Then Is a Slow Burn Look ...\n",
       "18    Snyder Cut Review: As Good as a Meh Avengers M...\n",
       "19    Review: 'Minari' tells the story of family, cu...\n",
       "20    The Falcon and the Winter Soldier Is a 1980s B...\n",
       "21    LG HU810PW projector review: Powerful color an...\n",
       "22    ‘Swan Song’: A Slight But Sweet Vehicle For Li...\n",
       "23    Review: Zack Snyder’s Justice League Is The Ul...\n",
       "24    Adios Is a Thought-Provoking but Quite Short N...\n",
       "25    ‘The Falcon and the Winter Soldier’ Review: Ma...\n",
       "26           Movie Review: Zack Snyder's Justice League\n",
       "27    Review: ‘Operation Varsity Blues’ recaps the s...\n",
       "28    Mumbai Saga movie review and release LIVE UPDA...\n",
       "29    ‘Justice League’ review: Zack Snyder’s film st...\n",
       "30    Mosagallu movie review: Vishnu Manchu, Kajal A...\n",
       "31    Review: 'Zack Snyder's Justice League' has arr...\n",
       "32    SXSW 2021: GAIA Review – Sporadically Frighten...\n",
       "33    Varthamanam movie review: Parvathy’s sweetness...\n",
       "34    Two Sisters Go on a Road Trip to Save Grandma ...\n",
       "35    Now Streaming: New Films And Series To Watch T...\n",
       "36                    Every Queen Latifah movie, ranked\n",
       "37    Review: Big heroes face small problems in 'The...\n",
       "38    'The Falcon and the Winter Soldier' review: Wi...\n",
       "39    Movie classics under review for racist and sex...\n",
       "40    Review: Tupac, Biggie and corruption in the LA...\n",
       "41    Endgame movie review: Andy Lau plays a killer ...\n",
       "42    British 'Justice League' fans baffled after br...\n",
       "43    The Sound of Metal Movie Review: Riz Ahmed is ...\n",
       "44    'Minari' is one of the top nominees at this ye...\n",
       "45                  'Mosagallu' Movie Review and Rating\n",
       "46    Mumbai Saga Movie Review: The John Abraham and...\n",
       "47    Zack Snyder’s Justice League Is A Better Movie...\n",
       "48                              Movie review: Come True\n",
       "49    The road to releasing Zack Snyder's 'Justice L...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8MKjQ5gC_Tw",
    "outputId": "123b3fa5-4305-456e-ad40-33dee5a1e130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.9988527  0.00114728]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-3.397287, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "tf.Tensor([[9.992549e-01 7.451388e-04]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-3.5379896, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.8820931  0.11790695]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-1.1021264, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.991621   0.00837898]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.4595747, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:7 out of the last 9 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[6.2001473e-04 9.9937999e-01]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.5912619, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:8 out of the last 10 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.9815859  0.01841402]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.0974076, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.0020442 0.9979558]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(2.9894981, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.04809644 0.9519035 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.3163056, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.00278613 0.9972139 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(2.770474, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.05680537 0.9431946 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.3015333, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.06317581 0.9368242 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.2290716, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.521832   0.47816804]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-0.19940186, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.8820931  0.11790695]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-1.1021264, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.9899569  0.01004312]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.3142352, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[0.00157534 0.99842465]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.0815895, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.07795523 0.92204475]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.0743141, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.05458585 0.9454142 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.3317792, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.00174026 0.9982597 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.1128926, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.00520343 0.9947966 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(2.4010572, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function TFBertMainLayer.call at 0x7f47a59fe8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([[7.8696036e-04 9.9921298e-01]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.4989917, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.10316521 0.8968348 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(0.91922426, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.08371723 0.9162828 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(0.95886946, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.0012904  0.99870956]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.2666771, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.00360146 0.99639857]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(2.6867514, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.00155522 0.99844486]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.2003822, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "tf.Tensor([[9.077642e-04 9.990922e-01]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.304809, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "tf.Tensor([[0.17744982 0.8225501 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(0.42858243, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.7811189  0.21888103]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-0.9272557, shape=(), dtype=float32)\n",
      "tf.Tensor([[3.6579565e-04 9.9963415e-01]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.8935966, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.9912226  0.00877742]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.4111426, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "tf.Tensor([[0.9908723  0.00912766]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.5823975, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.48984295 0.51015705]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-0.055116702, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.95998454 0.04001548]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-1.7734036, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.9932092 0.0067908]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.436291, shape=(), dtype=float32)\n",
      "tf.Tensor([[9.2628918e-04 9.9907374e-01]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.3651247, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "tf.Tensor([[0.01148587 0.9885141 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(2.0471144, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.1414884 0.8585116]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(0.72998667, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.07512854 0.92487144]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.0260904, shape=(), dtype=float32)\n",
      "tf.Tensor([[5.3808920e-04 9.9946195e-01]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.6209295, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.10859711 0.89140284]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.0026727, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.9981159  0.00188417]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-3.2990298, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.02053579 0.9794642 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.7312043, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.9950665 0.0049335]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.8641357, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.47314876 0.52685124]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-0.15133034, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "tf.Tensor([[0.04058346 0.95941657]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(1.6083932, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.13038714 0.8696128 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(0.70396614, shape=(), dtype=float32)\n",
      "tf.Tensor([[6.7487609e-04 9.9932516e-01]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(3.5171082, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.99275184 0.00724818]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(-2.6531243, shape=(), dtype=float32)\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "tf.Tensor([[0.00672586 0.9932741 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(2.3166425, shape=(), dtype=float32)\n",
      "tf.Tensor([[0.43661514 0.5633849 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(0.10963763, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for text in data.title:\n",
    "  tf_batch = tokenizer(text, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "  tf_outputs = model(tf_batch)\n",
    "  tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "  labels = ['Negative','Positive']\n",
    "  label = tf.argmax(tf_predictions, axis=1)\n",
    "  #label = label.numpy()\n",
    "  #for i in range(len(pred_sentences)):\n",
    "  rate = tf_predictions\n",
    "  score = tf_outputs.logits[0][1]\n",
    "  print(rate)\n",
    "  print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J46VaPJ2Ksd2"
   },
   "source": [
    "### create a function to get the score and  rate from models output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "VXMaTX76Mz5d"
   },
   "outputs": [],
   "source": [
    "\n",
    "def bert_Sent(text):\n",
    "  tf_batch = tokenizer(text, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "  tf_outputs = model(tf_batch)\n",
    "  tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "  #label = tf.argmax(tf_predictions, axis=1)\n",
    "  #label = label.numpy()\n",
    "  score = tf_outputs.logits[0][1]\n",
    "  return score\n",
    "\n",
    "import re\n",
    "\n",
    "def get_score(text):\n",
    "  score   = re.search('tf.Tensor\\((.+?), shape',text).group(1)\n",
    "  return score \n",
    "\n",
    "def get_rate(text):\n",
    "  tf_batch = tokenizer(text, max_length=100, padding=True, truncation=True, return_tensors='tf')\n",
    "  tf_outputs = model(tf_batch)\n",
    "  tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "  labels = ['Negative','Positive']\n",
    "  label = tf.argmax(tf_predictions, axis=1)\n",
    "  label = label.numpy()\n",
    "  return labels[label[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9IMY3vQLFaU"
   },
   "source": [
    "## Get the score a =n rate for all 50 rows and add them to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c5uKXkoC_Xm"
   },
   "outputs": [],
   "source": [
    "transf_sc  = [bert_Sent(text) for text in data.title]\n",
    "data['Bert_sentiment'] = transf_sc\n",
    "data['Bert_sentiment'] = data['Bert_sentiment'].astype(str)\n",
    "data['Bert_sentiment'] = [float(get_score(text)) for text in data.Bert_sentiment]\n",
    "\n",
    "tf_rate  = [get_rate(text) for text in data.title]\n",
    "data['Bert_rate'] = tf_rate \n",
    "\n",
    "data[['nltk_sentiment','TextBlob_sentiment', 'Bert_sentiment']] = data[['nltk_sentiment','TextBlob_sentiment', 'Bert_sentiment']].apply(lambda x: pd.Series.round(x,2)) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UzmUJfSPHI9t",
    "outputId": "821d56b2-aedc-4ac6-f435-d0f4493e3219"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afinn_sentimen</th>\n",
       "      <th>nltk_sentiment</th>\n",
       "      <th>TextBlob_sentiment</th>\n",
       "      <th>transformers_score</th>\n",
       "      <th>transformers_label</th>\n",
       "      <th>Bert_sentiment</th>\n",
       "      <th>Bert_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.99</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.46</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.59</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.10</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>2.99</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1.32</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>2.77</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.77</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1.30</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1.23</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.99</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.99</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.08</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1.07</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>1.33</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.11</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>2.40</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.77</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.92</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.96</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.27</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>2.69</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>3.20</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.30</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.43</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-5</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.99</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.89</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-3</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>-1.77</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.44</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.99</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.37</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>2.05</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.73</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>1.03</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.62</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-5</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-3</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.98</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1.73</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.86</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>1.61</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3.52</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>2.32</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.11</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    afinn_sentimen  nltk_sentiment  TextBlob_sentiment  transformers_score  \\\n",
       "0               -2           -0.40                0.40                1.00   \n",
       "1               -3           -0.84                0.00                0.96   \n",
       "2               -3            0.20               -0.18                0.99   \n",
       "3               -1           -0.05               -0.11                1.00   \n",
       "4               -2           -0.60               -0.20                1.00   \n",
       "5               -1           -0.20               -0.30                0.92   \n",
       "6                0           -0.30                0.00                1.00   \n",
       "7                4            0.68                0.28                1.00   \n",
       "8                2            0.46                0.57                1.00   \n",
       "9                2            0.15                0.50                0.77   \n",
       "10               0            0.00                0.00                1.00   \n",
       "11               2            0.53               -0.05                0.99   \n",
       "12              -3            0.20               -0.18                0.99   \n",
       "13               0            0.00                0.00                1.00   \n",
       "14              -1           -0.08                0.00                1.00   \n",
       "15               0           -0.57                0.00                0.96   \n",
       "16               0            0.23                0.15                0.99   \n",
       "17               0            0.00               -0.23                1.00   \n",
       "18               0            0.13                0.35                1.00   \n",
       "19              -2           -0.36                0.00                1.00   \n",
       "20               0            0.00                0.10                0.77   \n",
       "21               4            0.42                0.30                1.00   \n",
       "22               2            0.46                0.09                1.00   \n",
       "23               2            0.53                0.00                1.00   \n",
       "24              -1            0.00                0.20                0.91   \n",
       "25               5            0.65                0.17                1.00   \n",
       "26               2            0.53                0.00                0.99   \n",
       "27              -5           -0.76               -0.50                0.99   \n",
       "28               0            0.00                0.14                1.00   \n",
       "29              -1            0.23               -0.30                0.99   \n",
       "30               0            0.03                0.00                1.00   \n",
       "31               2            0.53               -0.05                0.89   \n",
       "32              -3           -0.78               -0.50                1.00   \n",
       "33              -5            0.38               -0.38                1.00   \n",
       "34               4            0.71                0.38                0.99   \n",
       "35               0            0.00                0.14                1.00   \n",
       "36               0            0.00                0.00                0.97   \n",
       "37               1            0.15               -0.12                0.97   \n",
       "38               1            0.32                0.10                1.00   \n",
       "39              -5           -0.61                0.00                1.00   \n",
       "40              -3           -0.42               -0.50                1.00   \n",
       "41               1           -0.20                0.05                0.98   \n",
       "42               1            0.32                0.00                1.00   \n",
       "43               1            0.27                0.70                1.00   \n",
       "44               2            0.20                0.50                1.00   \n",
       "45               0            0.00                0.00                0.97   \n",
       "46               0            0.00                0.00                1.00   \n",
       "47               7            0.85                0.60                1.00   \n",
       "48               2            0.42                0.35                1.00   \n",
       "49               1            0.32               -0.05                1.00   \n",
       "\n",
       "   transformers_label  Bert_sentiment Bert_rate  \n",
       "0            NEGATIVE           -3.40  Negative  \n",
       "1            NEGATIVE           -3.54  Negative  \n",
       "2            POSITIVE           -1.10  Negative  \n",
       "3            NEGATIVE           -2.46  Negative  \n",
       "4            POSITIVE            3.59  Positive  \n",
       "5            NEGATIVE           -2.10  Negative  \n",
       "6            POSITIVE            2.99  Positive  \n",
       "7            POSITIVE            1.32  Positive  \n",
       "8            POSITIVE            2.77  Positive  \n",
       "9            POSITIVE            1.30  Positive  \n",
       "10           POSITIVE            1.23  Positive  \n",
       "11           POSITIVE           -0.20  Negative  \n",
       "12           POSITIVE           -1.10  Negative  \n",
       "13           NEGATIVE           -2.31  Negative  \n",
       "14           POSITIVE            3.08  Positive  \n",
       "15           POSITIVE            1.07  Positive  \n",
       "16           NEGATIVE            1.33  Positive  \n",
       "17           POSITIVE            3.11  Positive  \n",
       "18           POSITIVE            2.40  Positive  \n",
       "19           POSITIVE            3.50  Positive  \n",
       "20           POSITIVE            0.92  Positive  \n",
       "21           POSITIVE            0.96  Positive  \n",
       "22           POSITIVE            3.27  Positive  \n",
       "23           POSITIVE            2.69  Positive  \n",
       "24           NEGATIVE            3.20  Positive  \n",
       "25           POSITIVE            3.30  Positive  \n",
       "26           POSITIVE            0.43  Positive  \n",
       "27           POSITIVE           -0.93  Negative  \n",
       "28           POSITIVE            3.89  Positive  \n",
       "29           NEGATIVE           -2.41  Negative  \n",
       "30           NEGATIVE           -2.58  Negative  \n",
       "31           POSITIVE           -0.06  Positive  \n",
       "32           POSITIVE           -1.77  Negative  \n",
       "33           NEGATIVE           -2.44  Negative  \n",
       "34           POSITIVE            3.37  Positive  \n",
       "35           POSITIVE            2.05  Positive  \n",
       "36           POSITIVE            0.73  Positive  \n",
       "37           NEGATIVE            1.03  Positive  \n",
       "38           POSITIVE            3.62  Positive  \n",
       "39           NEGATIVE            1.00  Positive  \n",
       "40           NEGATIVE           -3.30  Negative  \n",
       "41           POSITIVE            1.73  Positive  \n",
       "42           NEGATIVE           -2.86  Negative  \n",
       "43           NEGATIVE           -0.15  Positive  \n",
       "44           POSITIVE            1.61  Positive  \n",
       "45           POSITIVE            0.70  Positive  \n",
       "46           POSITIVE            3.52  Positive  \n",
       "47           NEGATIVE           -2.65  Negative  \n",
       "48           POSITIVE            2.32  Positive  \n",
       "49           NEGATIVE            0.11  Positive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n",
    "data[[ 'afinn_sentimen','nltk_sentiment','TextBlob_sentiment','transformers_score','transformers_label','Bert_sentiment','Bert_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "_5pAI5b8ah4f"
   },
   "outputs": [],
   "source": [
    "data.to_excel('/content/drive/MyDrive/Movie_Reviews_sentiments.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Movie_Review_ Bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
